{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f37bc793-99bb-4d18-9997-04e41ac5c6b8",
   "metadata": {},
   "source": [
    "# Sort by camera\n",
    "## 12/15/21\n",
    "Group all data in the 620 benthic concepts from 2000-2019 by which camera collected the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba2ada99-5f55-4879-9922-f24de5197864",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import datetime\n",
    "import requests\n",
    "import json\n",
    "import progressbar\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fathomnet.api import images\n",
    "import matplotlib.pyplot as plt\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from scipy.spatial.distance import braycurtis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e3a38f-d1bc-4587-ac17-50f91f9c7028",
   "metadata": {},
   "source": [
    "This will be sort of hacky as there currently is not a field in FathomNet for camera used to collect the image. But there are these date ranges for cameras deployed on each ROV.\n",
    "\n",
    "| rov_name | camera_name | start_date | end_date | fov_width_pixels | fov_height_pixels |\n",
    "| --- | --- | --- | --- | --- | --- |  \n",
    "| Doc Ricketts | Ikegami HDL40  | 2/10/2009 12:00:00 AM | 12/31/2019 11:59:00 PM | 1920 | 1080 |\n",
    "| Tiburon | Panasonic 3-chip | 10/29/1996 12:00:00 AM | 9/23/2005 11:59:00 PM | 720 | 480 |\n",
    "| Tiburon | Ikegami HDL40 | 10/4/2005 12:00:00 AM | 12/20/2007 11:59:00 PM | 1920 | 1080 |\n",
    "| Ventana | Sony 3-chip | 8/25/1988 12:00:00 AM | 8/25/1999 11:59:00 PM | 720 | 480 |\n",
    "| Ventana | Sony HDTV | 9/3/1999 12:00:00 AM | 7/9/2007 11:59:00 PM | 1920 | 1080 |\n",
    "| Ventana | Sony 3-chip | 7/11/2007 12:00:00 AM | 7/31/2007 11:59:00 PM | 720 | 480 |\n",
    "| Ventana | Ikegami HDL40 | 8/2/2007 12:00:00 AM | 12/31/2019 11:59:00 PM | 1920 | 1080 |\n",
    "\n",
    "First make the table into a dictonary and then load in the COCO json dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e554503-a4a2-41f2-8afd-192704a6f5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "cams = {'Doc%20Ricketts': {'ikegami': ['2009-02-10', '2019-12-31']},\n",
    "        'Tiburon': {'panasonic': ['1996-10-29', '2005-09-23'],\n",
    "                    'ikegami': ['2005-10-04', '2007-12-20']},\n",
    "        'Ventana': {'sonyChip': ['2007-07-11', '2007-07-31'],\n",
    "                    'sonyHD': ['1999-09-03', '2007-07-09'],\n",
    "                    'ikegami': ['2008-08-02', '2019-12-31']}\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "524742c4-c3c0-4bbf-a86a-d365592c5e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/eorenstein/Documents/fathomnet/duds/datasets/dataset.json', 'r') as ff:\n",
    "    anns = json.load(ff)\n",
    "    ff.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339737a4-89e0-4a89-ac14-e066f1f52436",
   "metadata": {},
   "source": [
    "Now pull out the url strings that are needed to retrieve the entire FathomNet AImageDTO object. Then split into separate lists based on vehicle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09dba5e6-6837-4e22-9293-c8f45121013a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ventana = 6757\n",
      "Ricketts = 18468\n",
      "Tiburon = 3166\n"
     ]
    }
   ],
   "source": [
    "urls = [(line['coco_url'], line['coco_url'].split('/')[-4]) for line in anns['images']]\n",
    "urls = list(set(urls))  # remove any duplicates \n",
    "\n",
    "vehicles = {}\n",
    "vehicles['vent'] = [line[0] for line in urls if line[1] == 'Ventana']\n",
    "vehicles['rick'] = [line[0] for line in urls if line[1] == 'Doc%20Ricketts']\n",
    "vehicles['tib'] = [line[0] for line in urls if line[1] == 'Tiburon']\n",
    "\n",
    "print('Ventana =', len(vehicles['vent']))\n",
    "print('Ricketts =', len(vehicles['rick']))\n",
    "print('Tiburon =', len(vehicles['tib']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41db610-5378-4770-b681-0c9a2cdeabae",
   "metadata": {},
   "source": [
    "Iterate over the urls to get the rest of the information we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6af9d434-6523-4970-8ee3-5f975db2fa60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (10 of 10) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    }
   ],
   "source": [
    "for vv in vehicles.keys():\n",
    "    for url in progressbar.progressbar(vehicles[vv]):\n",
    "            tmp = images.find_by_url(url) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3e5c4e-2737-401b-8806-f52d68e89455",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
